{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f000d6-ab22-4b9c-a714-621d039f8a79",
   "metadata": {},
   "source": [
    "# Siamese Network for Intrusion Detection\n",
    "- Dataset: NSL-KDD (https://www.unb.ca/cic/datasets/nsl.htmlhttps://www.unb.ca/cic/datasets/nsl.html)\n",
    "- Model: Siamese Net with Triplet-loss (https://arxiv.org/abs/1503.03832https://arxiv.org/abs/1503.03832)\n",
    "- Resuls: in-progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a362d66-c3a6-46a4-8443-bd2e4b24e9d1",
   "metadata": {},
   "source": [
    "## Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad7e016-dcb6-4f5e-98f8-cc1568c7f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "import os, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720dc049-27c6-4d17-87fb-0585e1e042de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NID_dataset_json={'Name':'Network Intrusion Detection',\n",
    "                  'path_train_csv':'dataset/Network_Intrusion_Detection/Train_data.csv',\n",
    "                  'path_test_csv': 'dataset/Network_Intrusion_Detection/Train_data.csv',\n",
    "                  'target_field':'class'\n",
    "                 }\n",
    "\n",
    "NSLKDD_dataset_json={'title':'NSL KDD',\n",
    "                     'path_train_csv':'dataset/NSL_KDD/NSL_KDD_Train.csv',\n",
    "                     'path_test_csv': 'dataset/NSL_KDD/NSL_KDD_Test.csv',\n",
    "                     'target_field':'class'\n",
    "                    }\n",
    "\n",
    "hyperparameter={'epoch':150,\n",
    "                'batch-size':1024,\n",
    "                'loss-function':'triplet_loss'\n",
    "               }\n",
    "\n",
    "\n",
    "os.mkdir('model') if not os.path.isdir('model') else None\n",
    "os.mkdir('model/checkpoint') if not os.path.isdir('model/checkpoint') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac24fd05-94cd-4048-8c8b-53c3b68aeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json=NID_dataset_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad034db-a48d-4084-8b55-957f6f2467c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3487f3d4-1c86-4bae-9f0b-25b00fd2fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading...\n",
      "\tFileloaded... dataset/Network_Intrusion_Detection/Train_data.csv\n",
      "Preprocessing...\n",
      "\tpre-shape:  (25192, 41) (25192,)\n",
      "\tnan coloumns: []\n",
      "\tcategorical coloumns: ['protocol_type', 'service', 'flag']\n",
      "\tpost-shape: (25192, 118) (25192,)\n",
      "Train/Test sets...\n",
      "\tTrain: (22672, 118) (22672,)\n",
      "\tTest: (2520, 118) (2520,)\n",
      "Reshape...done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dataset(path_csv):\n",
    "    df=pd.read_csv(path_csv)\n",
    "    \n",
    "    X = df.iloc[:, :-1] \n",
    "    y = df.iloc[:, -1]   \n",
    "    return X,y\n",
    "\n",
    "def create_batch(batch_size, x_dataset, y_dataset ):\n",
    "    feature_length=x_dataset.shape[0]\n",
    "    x_anchors = np.zeros((batch_size, feature_length))\n",
    "    x_positives = np.zeros((batch_size, feature_length))\n",
    "    x_negatives = np.zeros((batch_size, feature_length))\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        # We need to find an anchor, a positive example and a negative example\n",
    "        random_index = random.randint(0, x_dataset.shape[0] - 1)\n",
    "        x_anchor = x_dataset[random_index]\n",
    "        y = y_dataset[random_index]\n",
    "        \n",
    "        indices_for_pos = np.squeeze(np.where(y_dataset == y))\n",
    "        indices_for_neg = np.squeeze(np.where(y_dataset != y))\n",
    "        \n",
    "        x_positive = x_dataset[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]\n",
    "        x_negative = x_dataset[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]\n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]\n",
    "\n",
    "def preprocess_dataset(X,y):\n",
    "    print ('Preprocessing...')\n",
    "    shape_X=str(X.shape)\n",
    "    shape_y=str(y.shape)\n",
    "    nan_columns_X = [i for i in X.columns if X[i].isnull().any()]\n",
    "    categorical_columns_X=X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print (\"\\tpre-shape: \",shape_X,shape_y)\n",
    "    print (\"\\tnan coloumns:\",nan_columns_X)\n",
    "    print (\"\\tcategorical coloumns:\",categorical_columns_X)\n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean') imputer = imputer.fit(X[:, 1:])\n",
    "    # X[:, 1:] = imputer.transform(X[:, 1:])\n",
    "    \n",
    "    X=pd.get_dummies(data=X,columns=categorical_columns_X) # transform categorized to one-hot encoding\n",
    "    #y=pd.get_dummies(data=y)\n",
    "\n",
    "    # one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    # one_hot_encoder.fit(X,categorical_columns_X)\n",
    "    # X=one_hot_encoder.transform(X)\n",
    "    \n",
    "    label_encoder = LabelEncoder().fit(y)\n",
    "    y=label_encoder.transform(y)\n",
    "    \n",
    "\n",
    "    #X_train=X_train.values\n",
    "    shape_X=str(X.shape)\n",
    "    shape_y=str(y.shape)\n",
    "    print (\"\\tpost-shape:\",shape_X,shape_y)\n",
    "    \n",
    "    return X, y\n",
    " \n",
    "print ('Data loading...')\n",
    "X,y=dataset(dataset_json['path_train_csv'])\n",
    "print ('\\tFileloaded...',dataset_json['path_train_csv'])\n",
    "\n",
    "\n",
    "X,y=preprocess_dataset(X,y)\n",
    "\n",
    "print ('Train/Test sets...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "print ('\\tTrain:',X_train.shape, y_train.shape)\n",
    "print ('\\tTest:',X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "print ('Reshape...done')\n",
    "X_train = np.reshape(X_train.values,(X_train.values.shape[0], X_train.values.shape[1]) )/255.\n",
    "X_test = np.reshape(X_test.values,(X_test.values.shape[0], X_test.values.shape[1]) )/255.\n",
    "\n",
    "num_of_features=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987b8d40-4660-47e8-bd34-d96584113843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size,n_features, x_dataset,y_dataset):\n",
    "    x_anchors = np.zeros((batch_size, n_features))\n",
    "    x_positives = np.zeros((batch_size, n_features))\n",
    "    x_negatives = np.zeros((batch_size, n_features))\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        # We need to find an anchor, a positive example and a negative example\n",
    "        random_index = random.randint(0, x_dataset.shape[0] - 1)\n",
    "        x_anchor = x_dataset[random_index]\n",
    "        y = y_dataset[random_index]\n",
    "        \n",
    "        indices_for_pos = np.squeeze(np.where(y_dataset == y))\n",
    "        indices_for_neg = np.squeeze(np.where(y_dataset != y))\n",
    "        \n",
    "        x_positive = x_dataset[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]\n",
    "        x_negative = x_dataset[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]\n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]\n",
    "\n",
    "examples = create_batch(1,num_of_features,X_train,y_train)\n",
    "#print ('example:', (examples[0],examples[1],examples[2]))\n",
    "#print ('example:', (y_train[0],y_train[1],y_train[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec23907-f51c-425f-8884-17ca917f67e5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b4ff0f-3d60-401b-b780-2e4b7275bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                7616      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 11,776\n",
      "Trainable params: 11,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 15:11:03.331740: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "emb_size = 64\n",
    "\n",
    "embedding_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(num_of_features,)),\n",
    "    tf.keras.layers.Dense(emb_size, activation='sigmoid')\n",
    "])\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a368ab93-6235-4bcb-b747-3edb72ab0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "    return tf.maximum(positive_dist - negative_dist + alpha, 0.)\n",
    "\n",
    "def data_generator(batch_size,x_dataset,y_dataset):\n",
    "    while True:\n",
    "        x = create_batch(batch_size,num_of_features,x_dataset,y_dataset)\n",
    "        y = np.zeros((batch_size, 3*emb_size))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d093d6-782a-4ee5-b9de-51b0325ba685",
   "metadata": {},
   "source": [
    "### siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb00e637-4a4e-4989-a4a9-ebfbd845ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 118)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 118)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 118)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 64)           11776       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 11,776\n",
      "Trainable params: 11,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_anchor = tf.keras.layers.Input(shape=(num_of_features,))\n",
    "input_positive = tf.keras.layers.Input(shape=(num_of_features,))\n",
    "input_negative = tf.keras.layers.Input(shape=(num_of_features,))\n",
    "\n",
    "embedding_anchor = embedding_model(input_anchor)\n",
    "embedding_positive = embedding_model(input_positive)\n",
    "embedding_negative = embedding_model(input_negative)\n",
    "\n",
    "output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "model = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297a414-9944-4f16-b649-56b5f63c1aa6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb93ab8-1867-48d5-9015-2fd0a209bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n",
    "chk=tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model/checkpoint/',\n",
    "    monitor= \"loss\",\n",
    "    verbose = 0,\n",
    "    save_best_only  = True,\n",
    "    save_weights_only = True,\n",
    "    mode = \"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None,\n",
    "    initial_value_threshold=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ed28d0-0a65-41da-a08d-648cb9894735",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = hyperparameter['batch-size']\n",
    "epochs = hyperparameter['epoch']\n",
    "loss_function=hyperparameter['loss-function']#'triplet_loss'\n",
    "\n",
    "\n",
    "steps_per_epoch = int(X_train.shape[0]/batch_size)\n",
    "\n",
    "model.compile(loss=triplet_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feda70e7-0af7-4fb7-b7a5-68b7784a2edd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:AutoGraph could not transform <function triplet_loss at 0x150947f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpo0lfe7z3.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function triplet_loss at 0x150947f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpo0lfe7z3.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 15:11:03.981503: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 10s 462ms/step - loss: 0.1879\n",
      "Epoch 2/150\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.1655\n",
      "Epoch 3/150\n",
      "22/22 [==============================] - 11s 489ms/step - loss: 0.1299\n",
      "Epoch 4/150\n",
      "22/22 [==============================] - 10s 459ms/step - loss: 0.1013\n",
      "Epoch 5/150\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.0826\n",
      "Epoch 6/150\n",
      "22/22 [==============================] - 11s 481ms/step - loss: 0.0739\n",
      "Epoch 7/150\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.0664\n",
      "Epoch 8/150\n",
      "22/22 [==============================] - 11s 482ms/step - loss: 0.0615\n",
      "Epoch 9/150\n",
      "22/22 [==============================] - 10s 475ms/step - loss: 0.0586\n",
      "Epoch 10/150\n",
      "22/22 [==============================] - 10s 463ms/step - loss: 0.0584\n",
      "Epoch 11/150\n",
      "22/22 [==============================] - 11s 489ms/step - loss: 0.0578\n",
      "Epoch 12/150\n",
      "22/22 [==============================] - 11s 494ms/step - loss: 0.0556\n",
      "Epoch 13/150\n",
      "22/22 [==============================] - 10s 476ms/step - loss: 0.0535\n",
      "Epoch 14/150\n",
      "22/22 [==============================] - 11s 488ms/step - loss: 0.0516\n",
      "Epoch 15/150\n",
      "22/22 [==============================] - 11s 525ms/step - loss: 0.0504\n",
      "Epoch 16/150\n",
      "22/22 [==============================] - 11s 494ms/step - loss: 0.0506\n",
      "Epoch 17/150\n",
      "22/22 [==============================] - 10s 463ms/step - loss: 0.0486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    data_generator(batch_size, X_train,y_train),\n",
    "    #validation_data=data_generator(batch_size, X_test, y_test),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs, \n",
    "    callbacks=[es,chk],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abd381e-4a2e-41ef-83d0-49cb856c91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "#test_loss, test_acc = model.evaluate(data_generator(batch_size, X_test, y_test))\n",
    "#test_loss, test_acc = model.evaluate( X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f683d0e-aab3-4949-a56c-faf922810eb1",
   "metadata": {},
   "source": [
    "## Save/Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29084f-4d35-4e95-b8a4-ae85f283aa24",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d4cb9c-d122-4889-9ca8-de479114ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json =  model.to_json()\n",
    "with open(os.path.join('model',\"model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join('model',\"model.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d26b5-f309-4fa3-a687-a6f3d345c73e",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b733b7-31cc-4709-b8db-e635e45f462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open(os.path.join('model',\"model.json\"), \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(os.path.join('model',\"model.h5\"))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Evaluate\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73deab06-7ccb-48ec-a341-3053e3687643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
